{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "# standard library\n",
    "import gc\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import socket\n",
    "import statistics\n",
    "import struct\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# pip3 library\n",
    "import psutil\n",
    "import scipy\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.cluster as cluster\n",
    "import xgboost as xgb\n",
    "\n",
    "# custom functions\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')     # select plt style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gan_func\n",
    "importlib.reload(gan_func)  # For reloading after making changes\n",
    "from gan_func import *\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "# collect trash and check memory\n",
    "check_memory()\n",
    "\n",
    "# Load engineered dataset from EDA section\n",
    "dataSample = pd.read_csv('test.csv')\n",
    "print('Load Dataset')\n",
    "\n",
    "# save traffic\n",
    "normalized_dataSample = (dataSample - dataSample.mean()) / dataSample.std()\n",
    "\n",
    "data_cols = ['times', 'past']\n",
    "label_cols = ['Class']\n",
    "train = normalized_dataSample.copy()\n",
    "labels = cluster.KMeans(n_clusters=2, random_state=0).fit_predict(train[data_cols])\n",
    "print(dataSample.mean(), dataSample.std())\n",
    "print(pd.DataFrame([[np.sum(labels == i)] for i in np.unique(labels)], columns=['count'], index=np.unique(labels)))\n",
    "print('Data preprocessing finish')\n",
    "\n",
    "# Generate arguments and training the GAN and WGAN architectures, WGAN are use WGAN-GP\n",
    "print('Start to setting GAN')\n",
    "fraud_w_classes = train.copy()\n",
    "\n",
    "# print the original data figure\n",
    "origin = dataSample.values.copy()\n",
    "x2 = origin[:, 1]\n",
    "y2 = origin[:, 0]\n",
    "plt.scatter(x2, y2)\n",
    "plt.xlabel('times')\n",
    "plt.ylabel('cnt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Arguments\n",
    "rand_dim = 32                       # needs to be ~data_dim\n",
    "base_n_count = 128\n",
    "nb_steps = 25000 + 1                # Add one for logging of the last interval\n",
    "batch_size = 64\n",
    "k_d = 5                             # number of critic network updates per adversarial training step\n",
    "k_g = 1                             # number of generator network updates per adversarial training step\n",
    "critic_pre_train_steps = 100        # number of steps to pre-train the critic before starting adversarial training\n",
    "log_interval = 100                  # interval (in steps) at which to log loss summaries and save plots of image samples to disc\n",
    "learning_rate = 1e-4                # WGAN: 5e-5, WGAN-GP: 1e-4, CGAN: 2e-4\n",
    "data_dir = 'cache/'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)              # check dir is exist\n",
    "generator_model_path, discriminator_model_path, loss_pickle_path = None, None, None\n",
    "\n",
    "show = True                        # choose whether to show the traing img\n",
    "\n",
    "random_subset = fraud_w_classes.copy().reset_index(drop=True)                               # fraud only with labels from classification, reset the index\n",
    "arguments = [rand_dim, nb_steps, batch_size, k_d, k_g, critic_pre_train_steps, log_interval, learning_rate, base_n_count,\n",
    "             data_dir, generator_model_path, discriminator_model_path, loss_pickle_path, show]\n",
    "arguments[1] = 25001\n",
    "arguments[2] = 64\n",
    "\n",
    "# Four GANs\n",
    "# adversarial_training_GAN(arguments, random_subset, data_cols=data_cols)                             # GAN\n",
    "# adversarial_training_WGAN(arguments, train, data_cols=data_cols)                                    # WGAN\n",
    "# adversarial_training_InfoGAN(arguments, train, data_cols=data_cols)                                 # InfoGAN\n",
    "# adversarial_training_GAN(arguments, random_subset, data_cols=data_cols, label_cols=label_cols)      # CGAN\n",
    "adversarial_training_WGAN(arguments, train, data_cols=data_cols, label_cols=label_cols)             # WCGAN\n",
    "print('Finish training GAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some of the generated data\n",
    "# First create the networks locally and load the weights\n",
    "print('Create networks')\n",
    "seed = 17\n",
    "data_dim = len(data_cols)\n",
    "label_dim = len(label_cols)\n",
    "with_class = True if label_dim > 0 else False\n",
    "# with_class = False\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define four network models\n",
    "# with class\n",
    "generator_model, discriminator_model, combined_model = define_models_CGAN(rand_dim, data_dim, label_dim, base_n_count, type='Wasserstein')\n",
    "generator_model.load_weights('cache/WCGAN_generator_model_weights_step_' + str(nb_steps - 1) + '.h5')\n",
    "# generator_model, discriminator_model, combined_model = define_models_CGAN(rand_dim, data_dim, label_dim, base_n_count)\n",
    "# generator_model.load_weights('cache/CGAN_generator_model_weights_step_' + str(nb_steps - 1) + '.h5')\n",
    "\n",
    "# with_class = False\n",
    "# train = train_no_label\n",
    "# label_cols = []\n",
    "# generator_model, discriminator_model, combined_model = define_models_GAN(rand_dim, data_dim, base_n_count, type='Wasserstein')\n",
    "# generator_model.load_weights('cache/WGAN_generator_model_weights_step_' + str(nb_steps - 1) + '.h5')\n",
    "# generator_model, discriminator_model, combined_model = define_models_GAN(rand_dim, data_dim, base_n_count)\n",
    "# generator_model.load_weights('cache/GAN_generator_model_weights_step_' + str(nb_steps - 1) + '.h5')\n",
    "# generator_model, discriminator_model, combined_model = define_models_InfoGAN(rand_dim, data_dim, base_n_count)\n",
    "# generator_model.load_weights('cache/InfoGAN_generator_model_weights_step_' + str(nb_steps - 1) + '.h5')\n",
    "print('-------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate some new data\n",
    "print('Generate new data and result')\n",
    "test_size = int(dataSample.values.shape[0])             # Equal to all of the fraud cases, dataSample: 1554, test_size: 518\n",
    "\n",
    "# generate some data and use generator to get result\n",
    "x = get_data_batch(train, test_size, seed=seed)\n",
    "z = np.random.normal(size=(test_size, rand_dim))\n",
    "if with_class:\n",
    "    labels = x[:, :1]\n",
    "    g_z = generator_model.predict([z, labels])\n",
    "else:\n",
    "    g_z = generator_model.predict(z)\n",
    "\n",
    "# denormailize the generator's result\n",
    "result = g_z.copy()\n",
    "# print(labels)\n",
    "# print(result)\n",
    "result[:, 0] *= dataSample.std()[0]\n",
    "result[:, 0] += dataSample.mean()[0]\n",
    "\n",
    "result[:, 1] *= dataSample.std()[1]\n",
    "result[:, 1] += dataSample.mean()[1]\n",
    "\n",
    "b = result[:, 0].astype('int').reshape(-1, 1)\n",
    "c = result[:, 1].astype('float64').reshape(-1, 1)\n",
    "result = np.concatenate((b, c), axis=1)\n",
    "\n",
    "np.set_printoptions(suppress=True, formatter={'float_kind':'{:0.6f}'.format}) # setting print floating point numbers, and print format\n",
    "\n",
    "x1 = result[:, 1]\n",
    "y1 = result[:, 0]\n",
    "\n",
    "# print the generator data pic\n",
    "plt.scatter(x1, y1)\n",
    "plt.xlabel('times')\n",
    "plt.ylabel('cnt')\n",
    "save_name = data_dir + 'generator_fig' + '.png'\n",
    "plt.savefig(save_name)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "\n",
    "# print the original data figure\n",
    "origin = dataSample.values.copy()\n",
    "x2 = origin[:, 1]\n",
    "y2 = origin[:, 0]\n",
    "plt.scatter(x2, y2)\n",
    "plt.xlabel('times')\n",
    "plt.ylabel('cnt')\n",
    "save_name = data_dir + 'original_fig' + '.png'\n",
    "plt.savefig(save_name)\n",
    "plt.show()\n",
    "plt.cla()\n",
    "\n",
    "# show_info(x1, y1)\n",
    "# show_info(x2, y2)\n",
    "\n",
    "# generate some data and use generator to get result\n",
    "# run 10 times to generate data and result\n",
    "for i in range(10):\n",
    "    np.random.seed(i)\n",
    "    z = np.random.normal(size=(test_size, rand_dim))\n",
    "    labels = x[:,:1]\n",
    "    g_z = generator_model.predict([z, labels])\n",
    "    \n",
    "    # denormailize the generator's result\n",
    "    result = g_z.copy()\n",
    "    result[:, 0] *= dataSample.std()[0]\n",
    "    result[:, 0] += dataSample.mean()[0]\n",
    "\n",
    "    result[:, 1] *= dataSample.std()[1]\n",
    "    result[:, 1] += dataSample.mean()[1]\n",
    "\n",
    "    b = result[:, 0].astype('int').reshape(-1, 1)\n",
    "    c = result[:, 1].astype('float64').reshape(-1, 1)\n",
    "\n",
    "    result = np.concatenate((b, c), axis=1)\n",
    "\n",
    "    x1 = result[:, 1]\n",
    "    y1 = result[:, 0]\n",
    "\n",
    "    # print the generate data figure\n",
    "    plt.scatter(x1, y1)\n",
    "    plt.xlabel('times')\n",
    "    plt.ylabel('cnt')\n",
    "    save_name = data_dir + 'result_' + str(i + 1) + '_fig' + '.png'\n",
    "    plt.savefig(save_name)\n",
    "    plt.show()\n",
    "    plt.cla()\n",
    "print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gan_func\n",
    "importlib.reload(gan_func)  # For reloading after making changes\n",
    "from gan_func import *\n",
    "# load model\n",
    "print('Load model')\n",
    "base_dir = 'cache/'\n",
    "suffix = '_step_' + str(nb_steps - 1)\n",
    "\n",
    "# separate be combined_loss, disc_loss_real, disc_loss_generated, xgb_losses\n",
    "# GAN_losses = pickle.load(open(base_dir + 'GAN_losses' + suffix + '.pkl', 'rb'))\n",
    "# CGAN_losses = pickle.load(open(base_dir + 'CGAN_losses' + suffix + '.pkl', 'rb'))\n",
    "# WGAN_losses = pickle.load(open(base_dir + 'WGAN_losses' + suffix + '.pkl', 'rb'))\n",
    "WCGAN_losses = pickle.load(open(base_dir + 'WCGAN_losses' + suffix + '.pkl', 'rb'))\n",
    "\n",
    "# linestyles = ['-', '--', '-.', ':']\n",
    "losses = WCGAN_losses\n",
    "label = 'WCGAN'\n",
    "linestyle = ':'\n",
    "\n",
    "# Find best xgb scores overall and saved (every 100 steps)\n",
    "print('Find best xgb scores overall and saved')\n",
    "\n",
    "data_ix = 3\n",
    "# data_sets = [GAN_losses[data_ix], CGAN_losses[data_ix], WGAN_losses[data_ix], WCGAN_losses[data_ix]]\n",
    "# labels = ['GAN', 'CGAN', 'WGAN', 'WCGAN']\n",
    "data_set = losses[data_ix]\n",
    "\n",
    "best_step = list(data_set).index(np.array(data_set).min()) * 10\n",
    "print('{: <5} step {: <4}: {:.4f}'.format(label, best_step, np.array(data_set).min()))\n",
    "\n",
    "xgb100 = [data_set[i] for i in range(0, len(data_set), 10)]\n",
    "best_step = xgb100.index(min(xgb100)) * 100\n",
    "print('{: <5} step {: <4}: {:.4f}\\n'.format(label, best_step, np.array(xgb100).min()))\n",
    "# print(best_step, min(xgb100))\n",
    "\n",
    "\n",
    "# Look at the unsmoothed losses\n",
    "print('Look at the unsmoothed losses')\n",
    "unsmoothed_losses(losses, label, linestyle)\n",
    "unsmoothed_losses(losses, label, linestyle, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure for the critic losses for the WGAN and WCGAN\n",
    "print('Create a figure for the critic losses for the WGAN')\n",
    "w = 50       # Size of the moving window. This is the number of observations used for calculating the statistic.\n",
    "data_ix0 = 2 # generated_losses_\n",
    "data_ix1 = 1 # real_losses_\n",
    "# data_fields = ['combined_losses_', 'real_losses_', 'generated_losses_', 'xgb_losses']\n",
    "\n",
    "# labels = ['GAN','CGAN','WGAN','WCGAN'][1:3]\n",
    "# data_sets0 = [GAN_losses[data_ix0], CGAN_losses[data_ix0], WGAN_losses[data_ix0], WCGAN_losses[data_ix0]][1:3]\n",
    "# data_sets1 = [GAN_losses[data_ix1], CGAN_losses[data_ix1], WGAN_losses[data_ix1], WCGAN_losses[data_ix1]][1:3]\n",
    "# linestyles = ['-', '--', '-.', ':'][1:3]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "data0 = losses[data_ix0]\n",
    "data1 = losses[data_ix1]\n",
    "plt.plot(range(0, len(data0)), pd.DataFrame(np.array(data0) - np.array(data1)).rolling(w).mean(), label=label, linestyle=linestyle)\n",
    "plt.title('Difference between critic loss (EM distance estimate)\\non generated samples and real samples')\n",
    "plt.xlabel('training step')\n",
    "plt.ylabel('Gen - Real Critic Loss')\n",
    "legend = plt.legend() \n",
    "legend.get_frame().set_facecolor('white')\n",
    "plt.savefig('cache/Delta_critic_loss_plot.png')\n",
    "\n",
    "# save result\n",
    "np.savetxt('result.txt', result, newline='\\n', fmt='%0.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
